#!/usr/bin/env python3
# -*- coding: utf-8

import sys
import argparse
import subprocess
import csv
import os

"""what do I have and where I am going. 

I have a pipeline that works on a single sample. it takes a fastq file as input and creates all the specified subdirs and files
within a given dir.

What do I want: two fold objectives: I want to create a pipeline that is generalizable to all samples or a dataset. I would leverage 
argparse to do this and let argparse create a specified config file that snakemake ultimately uses. (implement argparse, create
config file from given data, update snakemake to work with config file) (Use dry run!)

The other objective is to create a pipeline that does subassemblies and iterative subtractive assemblies. I believe this can be 
accomplished with two new rules. One rule will be subsetting reads. another rule will be mapping reads. perhaps another rule
will be subsetting only unmapped reads.

TODO: set up argparse (done)
TODO: config file (done)
TODO: make config file hold resource req for each rule (testing)
    check config file is writen correctly
        make sample data/files (done)

        cmds
            conda activate snakemake
            full_assembly -i /Users/connerkojima/thrash/assembly_EAGER/test_data/samples/ -s _all.fastq -o /Users/connerkojima/thrash/assembly_EAGER/test_data/output_dir/ --resource-req /Users/connerkojima/thrash/assembly_EAGER/job_resources.yml -n
            

TODO: sub assemblies
    TODO: subsetting rule:
    TODO: mapping rule:
TODO: job groups see: https://stackoverflow.com/questions/69942132/snakemake-workflowerror-failed-to-group-jobs-together
    motivation: sub jobs won't delete their output if error in entire job group prevails--smarter slurm resource management


"""
def main():
    args = add_arguments()

    ref_args = preprocess_arguments(args)
    samples, filenames = ref_args['samples'], ref_args['filenames']
    snakefile_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "snakefile")
    profile_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "../.config/slurm/")
    config_filename = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../config_generated.yaml')
    create_config_file(config_filename, args, samples, filenames)
    dry_run_addon = ""
    if args.n:
        dry_run_addon = "-n"
    # subprocess and call snakemake with temp config file
    print(profile_path)
    subprocess.run("echo hello", shell=True)
    subprocess.run("snakemake -s {0} --profile {1} {2} --configfile {3} --debug-dag".format(snakefile_path, profile_path, dry_run_addon, config_filename), shell=True)
    # TODO: configure snakemake to use config file
    # TODO: delete config file 

def add_arguments():
    p = argparse.ArgumentParser(prog="Assembly workflow", description="Run Streamlined Assembly Protocols")
    p.add_argument('-i', help='Path to input interleaved fastq files. If path is a dir with fastq files, the -s flag should be used \n'
                   'i.e. "_all.fastq" for "<sample>_all.fastq". Another option is to indicate a csv file. The csv file should contain \n'
                   'sample names in the first column and paths to interleaved sample fastq files in the second column. ', required=True)
    p.add_argument('-s', help='suffix for interleaved fastq files i.e. "_all.fastq" for "<sample>_all.fastq"', required=False)
    p.add_argument('-o', help="path to output dir. If output dir does not exist, it will be created", required=True)
    p.add_argument('--resource-req', help='path to yaml file that stores the resource requirements if using slurm', required = False, dest='slurm_config')
    p.add_argument('-n', help='dry run through snakemake workflow', default=False, action='store_true')

    #TODO: add additional options to specify subassemblies and iterative subtractive assembly

    return p.parse_args()

def preprocess_arguments(args):
    """TODO: make sure -i flag points to dir or csv file. make sure that suffix flag is indicated if -i points to a dir"""
    ref_args = {}
    if args.slurm_config:
        with open(args.slurm_config, 'r') as f:
            lines = f.readlines()
            lines = ["  {0}".format(line) for line in lines]
        ref_args['slurm'] = lines
    
    if os.path.isdir(args.i):
        if not args.s:
            raise IOError("suffix must be supplied if input flag points to a directory")
        else:
            # get list of fastq files
            filenames = os.listdir(args.i)
            filenames = [name for name in filenames if args.s in name]
            samples = [sample.partition(args.s)[0] for sample in filenames]
            filenames = [os.path.join(args.i, name) for name in filenames]

            ref_args['samples'], ref_args['filenames'] = samples, filenames
    else:
        if args.s:
            raise IOError("suffix is supplied but input flag points to a file")
        else:
            # get list of fastq files
            samples, filenames = [], []
            with open(args.i, newline='') as csvfile:
                input_reader = csv.reader(csvfile, delimiter=',')
                for row in input_reader:
                    samples.append(row.split(",")[0])
                    filenames.append(row.split(",")[1])
            ref_args['samples'], ref_args['filenames'] = filenames
    return ref_args
        

def create_config_file(filename, args, samples, filenames):

    pairs = []
    for i in range(len(samples)):
        pairs.append("{0}: {1}".format(samples[i], filenames[i]))

    # string to write to config file
    yml_lines = """
    input:
      {0}
    output:
      output_dir: {1}
    slurm_resources: {2}
    """.format("\n      ".join(pairs), args.o, args.slurm_config)

    with open(filename, 'w') as f:
        f.writelines(yml_lines)


if __name__ == '__main__':
    main()