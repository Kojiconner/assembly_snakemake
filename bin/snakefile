import snakemake
import numpy
import os
import yaml

# make rules generalizable in terms of paths
# determine output structure ()
# make option to do just 100 percent assembly of all samples

fastq_dir='/project/thrash_89/db/EAGER_metaG_for_ck/interleaved_metaG/qual_trim_after_interleaving/brett_cat'
base_dir=config['output']['output_dir']

def determine_inputs(wildcards):
    inputs = {}
    if config['subiterassembly_percentages'] != 'none':
        inputs["subiter_assemblies"]=expand(base_dir + "{sample}_assembly/{subset}percent_ser/checkm/dastools/output_table.txt", sample=config['input'], subset=config['subiterassembly_percentages'][1:])
        inputs["subiterative_init"]=expand(base_dir + "{sample}_assembly/{subset}percent_par/checkm/dastools/output_table.txt", sample=config['input'], subset=config['subiterassembly_percentages'][0])
    if config['subassembly_percentages'] != 'none':
        inputs["subassemblies"]=expand(base_dir + "{sample}_assembly/{subset}percent_par/checkm/dastools/output_table.txt", sample=config['input'], subset=config['subassembly_percentages'])
    inputs['full']=expand(base_dir + "{sample}_assembly/100percent_par/checkm/dastools/output_table.txt", sample=config['input'])
    
    return inputs

rule all:
    input: 
        # full = expand(base_dir + "{sample}_assembly/100percent_par/checkm/dastools/output_table.txt", sample=config['input']),
        # unpack(determine_subassembly),
        unpack(determine_inputs)
        #subassemblies = expand(base_dir + "{sample}_assembly/{subset}percent_par/checkm/dastools/output_table.txt", sample=config['input'], subset=config['subassembly_percentages']),
        #subiterative = expand(base_dir + "{sample}_assembly/{subset}percent_ser/checkm/dastools/output_table.txt", sample=config['input'], subset=config['subassembly_percentages'][1:]),
        #subiterative_init = expand(base_dir + "{sample}_assembly/{subset}percent_par/checkm/dastools/output_table.txt", sample=config['input'], subset=config['subassembly_percentages'][0])
        
        #bam = expand(base_dir +'{sample}_assembly/{subset}percent_ser/subiterative_mapping/{sample}.bam', sample=config['input'], subset=config['subassembly_percentages'][2])
        #singles= expand(base_dir + '{sample}_assembly/100percent_par/sickle_trimmed/{sample}_all_singles.fastq', sample=config['input'])
        #paired_file = expand(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}_paired.txt', sample=config['input'])
        #contigs = expand(base_dir + '{sample}_assembly/full_par/spades_assembly/contigs.fasta', sample=config['input'])
        #expand(base_dir +"{sample}_assembly/100percent_par/binning/dastools/metabat_contigs2bin.tsv", sample=config['input'])
        # checkm_table = expand(base_dir + "{sample}_assemblly/100percent_par/checkm/dastools/output_table.txt", sample=config['input'])


# this sickle trim rule acts as a starting point (trims the 100% raw fastq fil)
rule sickle_trim_full:
    input:
        fastq = lambda wildcards: config['input'][wildcards.sample]
    output:
        singles=base_dir + '{sample}_assembly/100percentf_par/sickle_trimmed/{sample}_all_singles.fastq',
        trimmed=base_dir + '{sample}_assembly/100percentf_par/sickle_trimmed/{sample}_all_trimmed.fastq'
    conda: "../envs/sickle.yml"
    threads: 16
    resources:
        time = "00:20:00",
        mem_mb = 125000, #125GB
        partition = "epyc-64"
    shell:
        "sickle pe -c {input.fastq} -t sanger -m {output.trimmed} -s {output.singles}"

rule metaspades_assembly:
    input:
        trimmed_fq = base_dir + '{sample}_assembly/{subset}_{modifier}/sickle_trimmed/{sample}_all_trimmed.fastq'
    output:
        spades_dir = directory(base_dir + '{sample}_assembly/{subset}_{modifier}/spades_assembly/'),
        contigs = base_dir + '{sample}_assembly/{subset}_{modifier}/spades_assembly/contigs.fasta'
    conda: "../envs/spades.yml"
    threads: 64
    resources:
        time = "16:00:00",
        mem_mb = 998000, #998GB
        partition = "largemem"
    shell:
        "metaspades.py --12 {input} -o {output.spades_dir} --memory 998 -t {threads}"


rule bowtie2_map_reads:
    input:
        trimmed_fq = base_dir +'{sample}_assembly/{subset}_{modifier}/sickle_trimmed/{sample}_all_trimmed.fastq',
        contigs = base_dir +'{sample}_assembly/{subset}_{modifier}/spades_assembly/contigs.fasta'
    output:
        bam = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.bam',
        bam_index = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.bam.bai',
        sam = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.sam'),
        raw_bam = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}_RAW.bam'),
        index_f1 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.1.bt2'),
        index_f2 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.2.bt2'),
        index_f3 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.3.bt2'),
        index_f4 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.4.bt2'),
        index_r1 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.rev.1.bt2'),
        index_r2 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.rev.2.bt2')
    params: index = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}'
    conda: "../envs/read_mapping.yml"
    threads: 32
    resources:
        time = "1:00:00",
        mem_mb = 125000, #125GB
        partition = "epyc-64"
    shell:
        """bowtie2-build {input.contigs} {params.index} --threads {threads}

        # align reads
        bowtie2 -x {params.index} --interleaved {input.trimmed_fq} -S {output.sam} --threads {threads}

        # convert sam file into sorted and indexed bam file
        samtools view -bS {output.sam} --threads {threads} > {output.raw_bam}
        samtools sort {output.raw_bam} --threads {threads} > {output.bam}
        samtools index -@ {threads} {output.bam}"""


rule generate_depth_files:
    input:
        bam = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.bam'
    output:
        depth_file = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}_depth.txt',
        paired_file = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}_paired.txt'
    conda: "../envs/metabat2.yml"
    threads: 16
    resources:
        time = "2:00:00",
        mem_mb = 125000, #125GB,
        partition = "epyc-64"
    shell:
        "jgi_summarize_bam_contig_depths --outputDepth {output.depth_file} --pairedContigs {output.paired_file} {input.bam}"

rule bin_metabat2:
    input:
        contigs = base_dir +"{sample}_assembly/{subset}_{modifier}/spades_assembly/contigs.fasta",
        depth_file = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}_depth.txt'
    output:
        bins_dir = directory(base_dir +"{sample}_assembly/{subset}_{modifier}/binning/metabat2")
    conda: "../envs/metabat2.yml"
    params:
        minCVSum=0,
        minCV=0.1,
        m=2000
    threads: 16
    resources:
        time = "1:00:00",
        mem_mb = 125000, #125GB,
        partition = "epyc-64"
    shell:
        """
        mkdir {output.bins_dir} -p
        metabat2 -i {input.contigs}  -a {input.depth_file} -o {output.bins_dir}/metabat -t {threads} --minCVSum {params.minCVSum} --saveCls -d -v --minCV {params.minCV} -m {params.m}"""

rule bin_concoct:
    input:
        contigs = base_dir +"{sample}_assembly/{subset}_{modifier}/spades_assembly/contigs.fasta",
        bam = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.bam'
    output:
        contigs_bed = base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs/contigs_10K.bed",
        contig_chunks = base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs/contigs_10K.fa",
        cov_table = base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs/coverage_table.csv",
        clustering_gt1000 = base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs/concoct_subcontigs_clustering_gt1000.csv",
        clustering_merged = base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs/concoct_subcontigs_clustering_merged.csv",
        bins_dir = directory(base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs/fasta_bins"),
        concoct_dir = directory(base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs")
    conda: "../envs/concoct.yml"
    params:
        chunk=1000
    threads: 16
    resources:
        time = "24:00:00",
        mem_mb = 125000, #125GB,
        partition = "epyc-64"
    shell:
        """
        mkdir {output.bins_dir} -p 
        # first command (est time = 2 min)
        cut_up_fasta.py {input.contigs} -c {params.chunk} -o 0 --merge_last -b {output.contigs_bed} > {output.contig_chunks}
        # EST time = 4 hours
        concoct_coverage_table.py {output.contigs_bed} {input.bam} > {output.cov_table}

        # about 12 hours (overestimate) with 16 threads
        concoct --composition_file {output.contig_chunks} --coverage_file {output.cov_table} -b {output.concoct_dir}/concoct_subcontigs --threads {threads}

        merge_cutup_clustering.py {output.clustering_gt1000} > {output.clustering_merged}
        extract_fasta_bins.py {input.contigs} {output.clustering_merged} --output_path {output.bins_dir}"""

rule bin_maxbin2:
    input:
        contigs = base_dir +"{sample}_assembly/{subset}_{modifier}/spades_assembly/contigs.fasta",
        trimmed_fq = base_dir +'{sample}_assembly/{subset}_{modifier}/sickle_trimmed/{sample}_all_trimmed.fastq'
    output:
        bins_dir = directory(base_dir +"{sample}_assembly/{subset}_{modifier}/binning/maxbin2")
    log: 
        base_dir +"{sample}_assembly/{subset}_{modifier}/binning/maxbin2/maxbin.log"
    params:
        basename = base_dir +"{sample}_assembly/{subset}_{modifier}/binning/maxbin2/maxbin"
    conda: "../envs/maxbin2.yml"
    threads: 32
    resources:
        time = "6:30:00",
        mem_mb = 125000, #125GB,
        partition = "epyc-64"
    shell:
        """
        mkdir {output.bins_dir} -p
        run_MaxBin.pl -contig {input.contigs} -reads {input.trimmed_fq} -thread {threads} -out {params.basename}"""

rule generate_consensus_bins_dastools:
    input:
        contigs= base_dir +"{sample}_assembly/{subset}_{modifier}/spades_assembly/contigs.fasta",
        metabat_fa_dir= base_dir +"{sample}_assembly/{subset}_{modifier}/binning/metabat2",
        concoct_fa_dir= base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs",
        maxbin_fa_dir= base_dir +"{sample}_assembly/{subset}_{modifier}/binning/maxbin2"
    output:
        base_dir +"{sample}_assembly/{subset}_{modifier}/binning/dastools/metabat_contigs2bin.tsv",
        base_dir +"{sample}_assembly/{subset}_{modifier}/binning/dastools/maxbin_contigs2bin.tsv",
        base_dir +"{sample}_assembly/{subset}_{modifier}/binning/dastools/concoct_contigs2bin.tsv",
        dastools_dir=directory(base_dir +"{sample}_assembly/{subset}_{modifier}/binning/dastools"),
        dastools_bins=directory(base_dir +"{sample}_assembly/{subset}_{modifier}/binning/dastools/{sample}_DASTool_bins")
    conda: "../envs/dastool.yml"
    threads: 32
    resources:
        time = "2:30:00",
        mem_mb = 125000, #125GB,
        partition = "epyc-64"
    shell:
        """
        mkdir {output.dastools_bins} -p

        # metabat2
        Fasta_to_Contig2Bin.sh -e fa -i {input.metabat_fa_dir}/ > {output.dastools_dir}/metabat_contigs2bin.tsv

        # maxbin
        Fasta_to_Contig2Bin.sh -e fasta -i {input.maxbin_fa_dir}/ > {output.dastools_dir}/maxbin_contigs2bin.tsv

        # concoct (delete first line "contig_id	concoct.cluster_id", use correct command
        perl -pe "s/,/\\tconcoct./g;" {input.concoct_fa_dir}/concoct_subcontigs_clustering_merged.csv > {output.dastools_dir}/concoct_contigs2bin.tsv
        sed -i '1,1d' {output.dastools_dir}/concoct_contigs2bin.tsv

        # Run DAS_Tool (EST TIME < 2 hours per sample)
        DAS_Tool -i {output.dastools_dir}/maxbin_contigs2bin.tsv,\
{output.dastools_dir}/concoct_contigs2bin.tsv,\
{output.dastools_dir}/metabat_contigs2bin.tsv \
-l maxbin,concoct,metabat \
-c {input.contigs} \
-o {output.dastools_dir}/{wildcards.sample} --write_bins --write_bin_evals \
-t {threads}
        """

rule evaluate_bins_checkm:
    input:
        bins = base_dir +"{sample}_assembly/{subset}_{modifier}/binning/dastools/{sample}_DASTool_bins"
    output:
        checkm_dir = directory(base_dir +"{sample}_assembly/{subset}_{modifier}/checkm/dastools/"),
        checkm_table = base_dir + "{sample}_assembly/{subset}_{modifier}/checkm/dastools/output_table.txt"
    conda: "../envs/checkm.yml"
    threads: 16
    resources:
        time = "0:25:00",
        mem_mb = 120000, #120GB
        partition = "epyc-64"
    shell:
        """
        mkdir {output.checkm_dir} -p
        checkm lineage_wf -x fa -t {threads} {input.bins} {output.checkm_dir} -f {output.checkm_table}"""


# add subset percentage list
rule subset_reads_par:
    input:
        trimmed = base_dir + '{sample}_assembly/100percentf_par/sickle_trimmed/{sample}_all_trimmed.fastq'
    output: 
        trimmed = base_dir + '{sample}_assembly/{subset}percent_par/sickle_trimmed/{sample}_all_trimmed.fastq'
    shell:
        """
        reformat.sh in={input} out={output} samplerate={wildcards.subset} sampleseed=42
        """

# map assembly to reads
rule map_subassembly_reads:
    input:
        # TODO make dastool bin output lowest subset percentage that doesn't exist
        dastools_bin = base_dir + "{sample}_assembly/{subset}_{modifier}/binning/dastools/{sample}_DASTool_bins",
        trimmed_fq = base_dir + '{sample}_assembly/100percent_par/sickle_trimmed/{sample}_all_trimmed.fastq'
    output:
        cat_dastools_bins = base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/cat_consensus_bins.fa',
        bam = base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.bam',
        bam_index = base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.bam.bai',
        sam = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.sam'),
        raw_bam = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}_RAW.bam'),
        index_f1 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.1.bt2'),
        index_f2 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.2.bt2'),
        index_f3 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.3.bt2'),
        index_f4 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.4.bt2'),
        index_r1 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.rev.1.bt2'),
        index_r2 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.rev.2.bt2')
    params: index = base_dir +'{sample}_assembly/{subset}_{modifier}//subiterative_mapping/{sample}'
    conda: "../envs/read_mapping.yml"
    threads: 32
    resources:
        time = "1:00:00",
        mem_mb = 125000, #125GB
        partition = "epyc-64"
    shell:
        """
        # check strigency of mapped tag (TODO)

        # concatenate consensus/dastools bins and create index
        cat {input.dastools_bin}/*.fa > {output.cat_dastools_bins}
        bowtie2-build {output.cat_dastools_bins} {params.index} --threads {threads}

        # align reads (see how many reads(full set) align to the consensus bins)
        bowtie2 -x {params.index} --interleaved {input.trimmed_fq} -S {output.sam} --threads {threads}

        # convert sam file into sorted and indexed bam file
        # -f 4 discards mapped reads so that only unmapped reads are left
        samtools view -f 4 -bS {output.sam} --threads {threads} > {output.raw_bam}
        samtools sort {output.raw_bam} --threads {threads} > {output.bam}
        samtools index -@ {threads} {output.bam}"""

# put unmapped reads in fastq file
rule convert_bam_to_fastq:
    input: 
        bam = base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.bam'
    output:
        fastq = base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.fastq'
    shell:
        """
        samtools bam2fq {input.bam} > {output.fastq}

        """

 
def iterate_subset(wcs):
    n = wcs.percentage[:-7]
    # if output percentage is first subassembly percentage (which shouldn't exist)
    if n == config['subiterassembly_percentages'][0]:
        raise ValueError("bad snakemake")
    
    # if it is one of the other subassembly percentages
    for i in range(len(config['subiterassembly_percentages'])):
        if int(n) == config['subiterassembly_percentages'][i] or int(n) == 100:
            if i == 1:
                orig_percentage = config['subiterassembly_percentages'][i-1]
                return '%s%s_assembly/%d%s_%s/subiterative_mapping/%s.fastq' % (base_dir, wcs.sample, orig_percentage, 'percent', 'par', wcs.sample)
            else:
                orig_percentage = config['subiterassembly_percentages'][i-1]
                return '%s%s_assembly/%d%s_%s/subiterative_mapping/%s.fastq' % (base_dir, wcs.sample, orig_percentage, 'percent', 'ser', wcs.sample)
    return '%s%s_assembly/%d%s_%s/subiterative_mapping/%s.fastq' % (base_dir, wcs.sample, 100, 'percent', 'par', wcs.sample)
    

rule subset_reads_ser:
    input: 
        # e.g. 5_par
        iterate_subset
    output: 
        # e.g. 10_ser
        trimmed = base_dir + '{sample}_assembly/{percentage}_ser/sickle_trimmed/{sample}_all_trimmed.fastq'
    params: 
        percentage = lambda wc: wc.get("percentage")[:-7] 
        # https://stackoverflow.com/questions/71326692/passing-wildcard-values-in-params-in-snakemake
    shell:
        """
        reformat.sh in={input} out={output.trimmed} samplerate={params.percentage} sampleseed=42
        """