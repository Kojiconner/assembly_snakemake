import snakemake
import os
import yaml
import sys
import param_controller

# TODO make 100% subiter generalizable to all samples (with expand) deploy and test

fastq_dir='/project/thrash_89/db/EAGER_metaG_for_ck/interleaved_metaG/qual_trim_after_interleaving/brett_cat'
base_dir=config['output']['output_dir']

def determine_inputs(wildcards):
    inputs = {}
    if config['subassembly_percentages'] != 'none':
        inputs["subassemblies"]=expand(base_dir + "{sample}_assembly/{subset}percent_par/checkm/dastools/output_table.txt", sample=config['input'], subset=config['subassembly_percentages'])
    if config['full']:
        inputs['full'] = expand(base_dir + "{sample}_assembly/100percent_par/checkm/dastools/output_table.txt", sample=config['input'])
    if config['subiterassembly_percentages'] != 'none':
        inputs["subiter_assembly"]=expand(base_dir + "{sample}_assembly/{subset}percent_ser/checkm/dastools/output_table.txt", sample=config['input'], subset=config['subiterassembly_percentages'][-1])
    return inputs

rule all:
    input: 
        unpack(determine_inputs)

ruleorder: metaspades_assembly > subset_reads_par
ruleorder: subset_reads_ser > subset_reads_par
ruleorder: subset_reads_ser > evaluate_bins_checkm
ruleorder: subset_reads_ser > metaspades_assembly
ruleorder: convert_bam_to_fastq > evaluate_bins_checkm
ruleorder: map_subassembly_reads> convert_bam_to_fastq

ruleorder: generate_consensus_bins_dastools > subset_reads_par
ruleorder: generate_consensus_bins_dastools > subset_reads_ser
ruleorder: map_subassembly_reads > generate_consensus_bins_dastools
ruleorder: generate_consensus_bins_dastools > subset_reads_ser
ruleorder: generate_consensus_bins_dastools > convert_bam_to_fastq
ruleorder: generate_consensus_bins_dastools > metaspades_assembly
ruleorder: generate_consensus_bins_dastools > bin_metabat2
ruleorder: generate_consensus_bins_dastools > bin_concoct
ruleorder: generate_consensus_bins_dastools > bowtie2_map_reads
ruleorder: evaluate_bins_checkm > generate_consensus_bins_dastools
ruleorder: generate_consensus_bins_dastools > sickle_trim_full

ruleorder: generate_consensus_bins_dastools > sickle_trim_full

ruleorder: sickle_trim_full > subset_reads_par
ruleorder: sickle_trim_full > subset_reads_ser
ruleorder: sickle_trim_full > map_subassembly_reads
ruleorder: sickle_trim_full > subset_reads_ser
ruleorder: sickle_trim_full > convert_bam_to_fastq
ruleorder: sickle_trim_full > metaspades_assembly
ruleorder: sickle_trim_full > bin_metabat2
ruleorder: sickle_trim_full > bin_concoct
ruleorder: sickle_trim_full > bowtie2_map_reads
ruleorder: sickle_trim_full > generate_consensus_bins_dastools
ruleorder: generate_consensus_bins_dastools > sickle_trim_full

# this sickle trim rule acts as a starting point (trims the 100% raw fastq fil)
rule sickle_trim_full:
    input:
        fastq = lambda wildcards: config['input'][wildcards.sample]
    output:
        singles=base_dir + '{sample}_assembly/100percent_par/sickle_trimmed/{sample}_all_singles.fastq',
        trimmed=base_dir + '{sample}_assembly/100percent_par/sickle_trimmed/{sample}_all_trimmed.fastq'
    conda: "../envs/sickle.yml"
    threads: 16
    resources:
        time = param_controller.calculate_sickle_trim_full_time,
        mem_mb = param_controller.calculate_sickle_trim_full_mem,
        partition = param_controller.calculate_sickle_trim_full_partition
        
    shell:
        "sickle pe -c {input.fastq} -t sanger -m {output.trimmed} -s {output.singles}"

rule metaspades_assembly:
    input:
        trimmed_fq = base_dir + '{sample}_assembly/{subset}_{modifier}/sickle_trimmed/{sample}_all_trimmed.fastq'
    output:
        spades_dir = directory(base_dir + '{sample}_assembly/{subset}_{modifier}/spades_assembly/'),
        contigs = base_dir + '{sample}_assembly/{subset}_{modifier}/spades_assembly/contigs.fasta'
    conda: "../envs/spades.yml"
    threads: 64
    resources:
        time = param_controller.calculate_m_assembly_time,
        mem_mb = param_controller.calculate_m_assembly_mem,
        partition = param_controller.calculate_m_assembly_partition
    shell:
        "metaspades.py --12 {input} -o {output.spades_dir} --memory 998 -t {threads}"


rule bowtie2_map_reads:
    input:
        trimmed_fq = base_dir +'{sample}_assembly/{subset}_{modifier}/sickle_trimmed/{sample}_all_trimmed.fastq',
        contigs = base_dir +'{sample}_assembly/{subset}_{modifier}/spades_assembly/contigs.fasta'
    output:
        bam = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.bam',
        bam_index = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.bam.bai',
        sam = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.sam'),
        raw_bam = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}_RAW.bam'),
        index_f1 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.1.bt2'),
        index_f2 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.2.bt2'),
        index_f3 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.3.bt2'),
        index_f4 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.4.bt2'),
        index_r1 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.rev.1.bt2'),
        index_r2 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.rev.2.bt2')
    params: index = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}'
    conda: "../envs/read_mapping.yml"
    threads: 32
    resources:
        time = param_controller.calculate_bowtie2_time,
        mem_mb = 125000, #125GB
        partition = "epyc-64"
    shell:
        """bowtie2-build {input.contigs} {params.index} --threads {threads}

        # align reads
        bowtie2 -x {params.index} --interleaved {input.trimmed_fq} -S {output.sam} --threads {threads}

        # convert sam file into sorted and indexed bam file
        samtools view -bS {output.sam} --threads {threads} > {output.raw_bam}
        samtools sort {output.raw_bam} --threads {threads} > {output.bam}
        samtools index -@ {threads} {output.bam}"""


rule generate_depth_files:
    input:
        bam = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.bam'
    output:
        depth_file = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}_depth.txt',
        paired_file = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}_paired.txt'
    conda: "../envs/metabat2.yml"
    threads: 16
    resources:
        time = param_controller.calculate_g_depth_time,
        mem_mb = 125000, #125GB,
        partition = "epyc-64"
    shell:
        "jgi_summarize_bam_contig_depths --outputDepth {output.depth_file} --pairedContigs {output.paired_file} {input.bam}"

rule bin_metabat2:
    input:
        contigs = base_dir +"{sample}_assembly/{subset}_{modifier}/spades_assembly/contigs.fasta",
        depth_file = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}_depth.txt'
    output:
        bins_dir = directory(base_dir +"{sample}_assembly/{subset}_{modifier}/binning/metabat2")
    conda: "../envs/metabat2.yml"
    params:
        minCVSum=0,
        minCV=0.1,
        m=2000
    threads: 16
    resources:
        time = param_controller.calculate_metabat2_time,
        mem_mb = 125000, #125GB,
        partition = "epyc-64"
    shell:
        """
        mkdir {output.bins_dir} -p
        metabat2 -i {input.contigs}  -a {input.depth_file} -o {output.bins_dir}/metabat -t {threads} --minCVSum {params.minCVSum} --saveCls -d -v --minCV {params.minCV} -m {params.m}"""

rule bin_concoct:
    input:
        contigs = base_dir +"{sample}_assembly/{subset}_{modifier}/spades_assembly/contigs.fasta",
        bam = base_dir +'{sample}_assembly/{subset}_{modifier}/read_mapping/{sample}.bam'
    output:
        contigs_bed = base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs/contigs_10K.bed",
        contig_chunks = base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs/contigs_10K.fa",
        cov_table = base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs/coverage_table.csv",
        clustering_gt1000 = base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs/concoct_subcontigs_clustering_gt1000.csv",
        clustering_merged = base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs/concoct_subcontigs_clustering_merged.csv",
        bins_dir = directory(base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs/fasta_bins"),
        concoct_dir = directory(base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs")
    conda: "../envs/concoct.yml"
    params:
        chunk=1000
    threads: 16
    resources:
        time = param_controller.calculate_concoct_time,
        mem_mb = 125000, #125GB,
        partition = "epyc-64"
    shell:
        """
        mkdir {output.bins_dir} -p 
        # first command (est time = 2 min)
        cut_up_fasta.py {input.contigs} -c {params.chunk} -o 0 --merge_last -b {output.contigs_bed} > {output.contig_chunks}
        # EST time = 4 hours
        concoct_coverage_table.py {output.contigs_bed} {input.bam} > {output.cov_table}

        # about 12 hours (overestimate) with 16 threads
        concoct --composition_file {output.contig_chunks} --coverage_file {output.cov_table} -b {output.concoct_dir}/concoct_subcontigs --threads {threads}

        merge_cutup_clustering.py {output.clustering_gt1000} > {output.clustering_merged}
        extract_fasta_bins.py {input.contigs} {output.clustering_merged} --output_path {output.bins_dir}"""

rule bin_maxbin2:
    input:
        contigs = base_dir +"{sample}_assembly/{subset}_{modifier}/spades_assembly/contigs.fasta",
        trimmed_fq = base_dir +'{sample}_assembly/{subset}_{modifier}/sickle_trimmed/{sample}_all_trimmed.fastq'
    output:
        bins_dir = directory(base_dir +"{sample}_assembly/{subset}_{modifier}/binning/maxbin2")
    log: 
        base_dir +"{sample}_assembly/{subset}_{modifier}/binning/maxbin2/maxbin.log"
    params:
        basename = base_dir +"{sample}_assembly/{subset}_{modifier}/binning/maxbin2/maxbin"
    conda: "../envs/maxbin2.yml"
    threads: 32
    resources:
        time = param_controller.calculate_maxbin_time,
        mem_mb = 125000, #125GB,
        partition = "epyc-64"
    shell:
        """
        mkdir {output.bins_dir} -p
        run_MaxBin.pl -contig {input.contigs} -reads {input.trimmed_fq} -thread {threads} -out {params.basename}"""

rule generate_consensus_bins_dastools:
    input:
        contigs= base_dir +"{sample}_assembly/{subset}_{modifier}/spades_assembly/contigs.fasta",
        metabat_fa_dir= base_dir +"{sample}_assembly/{subset}_{modifier}/binning/metabat2",
        concoct_fa_dir= base_dir +"{sample}_assembly/{subset}_{modifier}/binning/concoct_subcontigs",
        maxbin_fa_dir= base_dir +"{sample}_assembly/{subset}_{modifier}/binning/maxbin2"
    output:
        base_dir +"{sample}_assembly/{subset}_{modifier}/binning/dastools/metabat_contigs2bin.tsv",
        base_dir +"{sample}_assembly/{subset}_{modifier}/binning/dastools/maxbin_contigs2bin.tsv",
        base_dir +"{sample}_assembly/{subset}_{modifier}/binning/dastools/concoct_contigs2bin.tsv",
        dastools_dir=directory(base_dir +"{sample}_assembly/{subset}_{modifier}/binning/dastools"),
        dastools_bins=directory(base_dir +"{sample}_assembly/{subset}_{modifier}/binning/dastools/{sample}_DASTool_bins")
    conda: "../envs/dastool.yml"
    threads: 32
    resources:
        time = param_controller.calculate_dastools_time,
        mem_mb = 125000, #125GB,
        partition = "epyc-64"
    shell:
        """
        mkdir {output.dastools_bins} -p

        # metabat2
        Fasta_to_Contig2Bin.sh -e fa -i {input.metabat_fa_dir}/ > {output.dastools_dir}/metabat_contigs2bin.tsv

        # maxbin
        Fasta_to_Contig2Bin.sh -e fasta -i {input.maxbin_fa_dir}/ > {output.dastools_dir}/maxbin_contigs2bin.tsv

        # concoct (delete first line "contig_id	concoct.cluster_id", use correct command
        perl -pe "s/,/\\tconcoct./g;" {input.concoct_fa_dir}/concoct_subcontigs_clustering_merged.csv > {output.dastools_dir}/concoct_contigs2bin.tsv
        sed -i '1,1d' {output.dastools_dir}/concoct_contigs2bin.tsv

        # Run DAS_Tool (EST TIME < 2 hours per sample)
        DAS_Tool -i {output.dastools_dir}/maxbin_contigs2bin.tsv,\
{output.dastools_dir}/concoct_contigs2bin.tsv,\
{output.dastools_dir}/metabat_contigs2bin.tsv \
-l maxbin,concoct,metabat \
-c {input.contigs} \
-o {output.dastools_dir}/{wildcards.sample} --write_bins --write_bin_evals \
-t {threads}
        """


checkpoint evaluate_bins_checkm:
    input:
        bins = base_dir +"{sample}_assembly/{subset}_{modifier}/binning/dastools/{sample}_DASTool_bins"
    output:
        checkm_dir = directory(base_dir +"{sample}_assembly/{subset}_{modifier}/checkm/dastools/"),
        checkm_table = base_dir + "{sample}_assembly/{subset}_{modifier}/checkm/dastools/output_table.txt",
        checkm_table_thresholded = base_dir + "{sample}_assembly/{subset}_{modifier}/checkm/dastools/threshold_table.txt"
    conda: "../envs/checkm.yml"
    threads: 16
    resources:
        time = param_controller.calculate_checkm_time,
        mem_mb = 120000, #120GB
        partition = "epyc-64"
    shell:
        """
        mkdir {output.checkm_dir} -p

        if [ -e "{input.bins}/*.fa" ]
        then
            echo "empty" > {output.checkm_table}
        else
            checkm lineage_wf -x fa -t {threads} {input.bins} {output.checkm_dir} -f {output.checkm_table}
        fi

        #initialize count variable
        count=0

        # NOTE: added double curly braces due to snakemake syntax in unix for loop
        # loop through checkm output table, skipping first three lines
        # count number of bins that meet threshold
        {{
        read; read; read;
        while IFS="" read -r p || [ -n "$p" ] ; do
            values=($p)
            if [[ $p = \ * ]] 
            then
                if (( $(echo "${{values[12]}} > 80.00" | bc -l)))
                then
                    count=$(($count + 1))
                fi
            fi
        done
        }} < {output.checkm_table}
        echo $count > {output.checkm_table_thresholded}
        """


# add subset percentage list
rule subset_reads_par:
    input:
        trimmed = base_dir + '{sample}_assembly/100percent_par/sickle_trimmed/{sample}_all_trimmed.fastq'
    output: 
        trimmed = base_dir + '{sample}_assembly/{subset}percent_par/sickle_trimmed/{sample}_all_trimmed.fastq'
    resources:
        time = param_controller.calculate_subset_par_time,
        mem_mb = 120000, #120GB
        partition = "main"
    params: 
        percentage = lambda wc: "{:02d}".format(int(wc.subset)) 
    conda: "../envs/bbmap.yml"
    shell:
        """
        reformat.sh in={input} out={output} samplerate=.{params.percentage} sampleseed=42
        """

def map_subassembly_reads_input(wcs):
    inputs = {}
    inputs['dastools_bin'] = '%s%s_assembly/%s_%s/binning/dastools/%s_DASTool_bins' % (base_dir, wcs.sample, wcs.subset, wcs.modifier, wcs.sample)
    inputs['checkm'] = '%s%s_assembly/%s_%s/checkm/dastools/output_table.txt' % (base_dir, wcs.sample, wcs.subset, wcs.modifier)

    # set n to to the output percentage
    n = wcs.subset[:-7]

    # determine where to go from par to ser (i.e. earliest thresholded file that isn't "0")
    # start_idx is earliest file that isn't 0 (has files that meet threshold)
    start_idx = len(config['subiterassembly_percentages']) - 1
    # count backwards
    for j in range(len(config['subiterassembly_percentages'])-1, 0, -1):
        with checkpoints.evaluate_bins_checkm.get(sample=wcs.sample, subset="{}percent".format(config['subiterassembly_percentages'][j]), modifier='par').output[2].open() as f:
            if f.read().strip() != "0":
                start_idx = j
    #checkpoint
    # loop through percentages
    for i in range(len(config['subiterassembly_percentages'])):
        if int(n) == config['subiterassembly_percentages'][i] or int(n) == 100:
            with checkpoints.evaluate_bins_checkm.get(sample=wcs.sample, subset="{}percent".format(config['subiterassembly_percentages'][i-1]), modifier='par').output[2].open() as f:
                #orig percentage is percentage before output (i.e. 20 --> 10, 50 --> 20)
                if i == (start_idx) and wcs.modifier == "par":
                # for the first number, subset from 100 percent reads (par to par)
                    inputs['trimmed_fq'] = '%s%s_assembly/100percent_par/sickle_trimmed/%s_all_trimmed.fastq' % (base_dir, wcs.sample, wcs.sample)
                # going from ser to ser (e.g. continuation )
                elif wcs.modifier == "ser" and i > start_idx + 1:
                # for the second number, subset from first subassembly (par to ser)
                    orig_percentage = config['subiterassembly_percentages'][i-1]
                    inputs['trimmed_fq'] = '%s%s_assembly/%dpercent_%s/subiterative_mapping/%s.fastq' % (base_dir, wcs.sample, orig_percentage, 'ser', wcs.sample)
                elif wcs.modifier == "ser" and i == start_idx + 1:
                    # input: /project/thrash_89/db/EAGER_metaG_for_ck/pipeline_assemblies/LKB_V4_S16_assembly/5percent_par/subiterative_mapping/LKB_V4_S16.fastq
                    # for the second number, subset from first subassembly (par to ser)
                    orig_percentage = config['subiterassembly_percentages'][i-1]
                    #LKB_V4_S16_assembly/5percent_par/subiterative_mapping/LKB_V4_S16.fastq'
                    inputs['trimmed_fq'] = '/project/thrash_89/db/EAGER_metaG_for_ck/pipeline_assemblies/LKB_V4_S16_assembly/5percent_par/subiterative_mapping/LKB_V4_S16.fastq'
                    inputs['trimmed_fq'] = '%s%s_assembly/%dpercent_%s/subiterative_mapping/%s.fastq' % (base_dir, wcs.sample, orig_percentage, 'par', wcs.sample)
                    # inputs['trimmed_fq'] = '%s%s_assembly/%dpercent_%s/subiterative_mapping/%s.fastq' % (base_dir, wcs.sample, orig_percentage, 'par', wcs.sample)
            if int(n) == 100:
            # special case for last subiterative assembly (should not return anything)
                inputs['trimmed_fq'] = 'none'
            # if int(n) == 10:
            #     # input: /project/thrash_89/db/EAGER_metaG_for_ck/pipeline_assemblies/LKB_V4_S16_assembly/5percent_par/subiterative_mapping/LKB_V4_S16.fastq
            #     # for the second number, subset from first subassembly (par to ser)
            #     orig_percentage = 5
            #     #LKB_V4_S16_assembly/5percent_par/subiterative_mapping/LKB_V4_S16.fastq'
            #     inputs['trimmed_fq'] = '/project/thrash_89/db/EAGER_metaG_for_ck/pipeline_assemblies/LKB_V4_S16_assembly/5percent_par/subiterative_mapping/LKB_V4_S16.fastq'
            #     inputs['trimmed_fq'] = '%s%s_assembly/%dpercent_%s/subiterative_mapping/%s.fastq' % (base_dir, wcs.sample, orig_percentage, 'par', wcs.sample)
            #     # inputs['trimmed_fq'] = '%s%s_assembly/%dpercent_%s/subiterative_mapping/%s.fastq' % (base_dir, wcs.sample, orig_percentage, 'par', wcs.sample)
    return inputs
    

# map assembly to reads
################
rule map_subassembly_reads:
    input:
        unpack(map_subassembly_reads_input)
        # dastools_bin = base_dir + "{sample}_assembly/{subset}_{modifier}/binning/dastools/{sample}_DASTool_bins",
        # trimmed_fq = base_dir + '{sample}_assembly/100percent_par/sickle_trimmed/{sample}_all_trimmed.fastq',
        # checkm = base_dir + "{sample}_assembly/{subset}_{modifier}/checkm/dastools/output_table.txt"
    output:
        cat_dastools_bins = base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/cat_consensus_bins.fa',
        bam = base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.bam',
        bam_index = base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.bam.bai',
        sam = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.sam'),
        raw_bam = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}_RAW.bam'),
        index_f1 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.1.bt2'),
        index_f2 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.2.bt2'),
        index_f3 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.3.bt2'),
        index_f4 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.4.bt2'),
        index_r1 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.rev.1.bt2'),
        index_r2 = temp(base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.rev.2.bt2')
    params: index = base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}'
    conda: "../envs/read_mapping.yml"
    threads: 32
    resources:
        time = param_controller.calculate_map_subassembly_time,
        mem_mb = 125000, #125GB
        partition = "epyc-64"
    shell:
        """
        # NOTE: added double curly braces due to snakemake syntax in unix for loop
        # loop through checkm output table, skipping first three lines
        # concatenate consensus/dastools bins for 80.00 and up completeness bins
        {{
        read; read; read;
        while IFS="" read -r p || [ -n "$p" ] ; do
            values=($p)
            if [[ $p = \ * ]] 
            then
                if (( $(echo "${{values[12]}} > 80.00" | bc -l)))
                then
                    cat {input.dastools_bin}/"${{values[0]}}.fa" > {output.cat_dastools_bins}
                    echo ${{values[0]}};
                fi
            fi
        done
        }} < {input.checkm}

        # create index
        bowtie2-build {output.cat_dastools_bins} {params.index} --threads {threads}

        # align reads (see how many reads(full set) align to the consensus bins)
        bowtie2 -x {params.index} --interleaved {input.trimmed_fq} -S {output.sam} --threads {threads}

        # convert sam file into sorted and indexed bam file
        # -f 12 keeps reads where both read and mate are unmapped
        samtools view -f 12 -bS {output.sam} --threads {threads} > {output.raw_bam}
        samtools sort {output.raw_bam} --threads {threads} > {output.bam}
        samtools index -@ {threads} {output.bam}"""

# put unmapped reads in fastq file
rule convert_bam_to_fastq:
    input: 
        bam = base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.bam'
    output:
        fastq = base_dir +'{sample}_assembly/{subset}_{modifier}/subiterative_mapping/{sample}.fastq'
    conda: "../envs/read_mapping.yml"
    resources:
        time = param_controller.calculate_convert_bam_time,
        mem_mb = 120000, #120GB
        partition = "epyc-64"
    shell:
        """
        samtools bam2fq {input.bam} > {output.fastq}
        """

 
def iterate_subset(wcs):
    # set n to to the output percentage
    n = wcs.percentage[:-7]

    for i in range(len(config['subiterassembly_percentages'])):
        if int(n) == config['subiterassembly_percentages'][i] or int(n) == 100:
            # check if checkm returned empty or a file
            with checkpoints.evaluate_bins_checkm.get(sample=wcs.sample, subset="{}percent".format(config['subiterassembly_percentages'][i-2]), modifier='par').output[2].open() as f:
                if f.read().strip() == "0":
                    # for the second number (i.e. 5 out of 1,5,10,20,50,100)
                    # allow to go from par to ser
                    # if the second subassembly percentage
                    orig_percentage = config['subiterassembly_percentages'][i-1]
                    return '%s%s_assembly/%d%s_%s/subiterative_mapping/%s.fastq' % (base_dir, wcs.sample, orig_percentage, 'percent', 'par', wcs.sample)
            if int(n) == 100:
            # for the second number (i.e. 5 out of 1,5,10,20,50,100)
            # allow to go from par to ser
                # if the second subassembly percentage
                orig_percentage = config['subiterassembly_percentages'][-2]
                return '%s%s_assembly/%d%s_%s/subiterative_mapping/%s.fastq' % (base_dir, wcs.sample, orig_percentage, 'percent', 'ser', wcs.sample)
            elif i == 1:
            # for the second number (i.e. 5 out of 1,5,10,20,50,100)
            # allow to go from par to ser
                # if the second subassembly percentage
                orig_percentage = config['subiterassembly_percentages'][i-1]
                return '%s%s_assembly/%d%s_%s/subiterative_mapping/%s.fastq' % (base_dir, wcs.sample, orig_percentage, 'percent', 'par', wcs.sample)
            elif int(n) != config['subiterassembly_percentages'][-1]:
            # for everything else except the last number
            # only go from ser to ser
                # otherwise, go from 
                orig_percentage = config['subiterassembly_percentages'][i-1]
                return '%s%s_assembly/%d%s_%s/subiterative_mapping/%s.fastq' % (base_dir, wcs.sample, orig_percentage, 'percent', 'ser', wcs.sample)
    

rule subset_reads_ser:
    input: 
        # e.g. 5_par
        iterate_subset
    output: 
        # e.g. 10_ser
        trimmed = base_dir + '{sample}_assembly/{percentage}_ser/sickle_trimmed/{sample}_all_trimmed.fastq'
    conda: "../envs/bbmap.yml"
    resources:
        time = param_controller.subset_reads_ser,
        mem_mb = 120000, #120GB
        partition = "epyc-64"
    params: 
        percentage = lambda wc: "{:02d}".format(int(wc.get("percentage")[:-7])) 
        # https://stackoverflow.com/questions/71326692/passing-wildcard-values-in-params-in-snakemake
    shell:
        """
        reformat.sh in={input} out={output.trimmed} samplerate=.{params.percentage} sampleseed=42
        """

